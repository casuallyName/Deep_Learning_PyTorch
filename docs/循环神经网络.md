# 循环神经网络

## 结构



![image-20200229114559878](D:\Deep_Learning_PyTorch\docs\image\image-20200229111907225.png)

* $X_t$是时间t处的输入:
* $S_t$是时间t处的“记忆”，$S_t=f(UX_t+WS_{t−1})$，$f$ 可以是$tanh$等

  * $X_t$为当前时间输入

  * ${S_{t-1}}$为截止上一时间点为止的memory
* $X$与$S$的规模可以不同
* 可以把隐状态$S_t$视作“记忆体”，捕捉了之前时间点上的信息。
* 很可惜，实际应用中，$S_t$并不能捕捉和保留之前所有信息（记忆有限，可能是梯度弥散）
* 参数共享，不同于CNN，RNN整个神经网络都共享一组参数（U,V,W）（整合方式相同），极大减小了需要训练和预估的参数量
* $O_t$是时间  $t$ 时的输出，比如是预测下个词的话，可能是$softmax$输出的属于每个候选词的概率，$O_t =softmax(VS_t)$
* 输出$O_t$由当前时间及之前所有的“记忆”共同计算得到，和当前时间点输入无关
* $O_t$在有些任务下是不存在的，比如文本情感分析，只需要最后的output结果就行

# 不同类型的RNN

## 双向RNN

![image-20200229123230817](D:\Deep_Learning_PyTorch\docs\image\image-20200229123230817.png)

* 两个方向$h_t$ 处理方向不同

* 有些情况下，当前的输出不只依赖于之前的序列元素，还可能依赖之后的序列元素（由后面的内容推导前面的内容）
* 直观理解：双向RNN叠加

## 深层双向RNN

![image-20200229123738513](D:\Deep_Learning_PyTorch\docs\image\image-20200229123738513.png)

* 横向可以理解为不同时间，纵向可以理解为1,2,3轮
* $f$可理解为：上一轮的memory与上一时间节点的memory做融合

# RNN中的算法

## BPTT算法

![image-20200229124251345](D:\Deep_Learning_PyTorch\docs\image\image-20200229124251345.png)

* BPTT和BP算法是一个思路，但BPTT与时间$t$ 有关系

![image-20200229124421687](D:\Deep_Learning_PyTorch\docs\image\image-20200229124421687.png)

​		

损失函数：

$E_t(y_t,\hat{y}_t)=-y_tlog\hat{y}_t$    交叉熵损失

$E(y_t,\hat{y}_t)=\sum_{t}E_t(y_t,\hat{y}_t)$    # 总的的交叉熵

​			 	$=-\sum_ty_tlog\hat{y}_t$

$\frac{\partial{E}}{\partial{W}}=\sum_t\frac{\partial{E_t}}{\partial{w}}$

$\frac{\partial{E_3}}{\partial{W}}=\frac{\partial{E_3}}{\partial{\hat{y}_3}}\frac{\partial{\hat{y}_3}}{\partial{s_3}}\frac{\partial{s_3}}{\partial{W}}$

$s_3=tanh(Ux_t+Ws_2)$,又依赖于$s_2$,

$\frac{\partial{E_3}}{\partial{W}}=\sum_{k=0}^{3}\frac{\partial{E_3}}{\partial{\hat{y}_3}}\frac{\partial{\hat{y}_3}}{\partial{s_3}}\frac{\partial{s_3}}{\partial{s_k}}\frac{\partial{s_k}}{\partial{W}}$

![image-20200229131940856](D:\Deep_Learning_PyTorch\docs\image\image-20200229131940856.png)

$\frac{\partial{E_3}}{\partial{W}}=\sum_{k=0}^{3}\frac{\partial{E_3}}{\partial{\hat{y}_3}}\frac{\partial{\hat{y}_3}}{\partial{s_3}}(\prod_{j=k+1}^3\frac{\partial{s_j}}{\partial{s_{j-1}}})\frac{\partial{s_k}}{\partial{W}}$

激活函数：$tanh$ 可以控制幅度，约束范围

![img](D:\Deep_Learning_PyTorch\docs\image\img0)

* 可以控制幅度

# 应用

图像描述

![image-20200229132918812](D:\Deep_Learning_PyTorch\docs\image\image-20200229132918812.png)

VGG  ==> $s_t=tanh(W{xh}*x+W_{hh}*h+W_{ih}*v)$

$s_t=tanh(W{xh}*x+W_{hh}*h)$  与 $s_t=tanh(W{xh}*x+W_{hh}*h+W_{ih}*v)$

 过去$s_t$基于$x_t$与$s_{t-1}$得到,现在基于$x_t$、$s_{t-1}$和图像的输入$v$得到

# LSTM

* RNN结构

![image-20200229135136194](D:\Deep_Learning_PyTorch\docs\image\image-20200229134925508.png)

* LSTM结构

![image-20200229135159363](D:\Deep_Learning_PyTorch\docs\image\image-20200229135159363.png)

## LSTM   细胞状态

### 信息传递交互

![image-20200229135750368](D:\Deep_Learning_PyTorch\docs\image\image-20200229135750368.png)

### 信息选择

![image-20200229135903827](D:\Deep_Learning_PyTorch\docs\image\image-20200229135903827.png)

### LSTM的几个关键“门”与操作

#### 1 决定从“细胞状态”中丢弃什么信息=> “忘记门”

![image-20200229140449300](D:\Deep_Learning_PyTorch\docs\image\image-20200229140449300.png)

* $\sigma$ 为$sigmoid$函数
* 通过“门”让信息选择性通过，来去除或者增加信息到细胞状态

* 包含一个Sigmoid神经网络层和一个$pointwise$乘法操作
* Sigmoid 层输出0到1之间的概率值，描述每个部分有多少量可以通过。0代表“不许任何量通过”，1就指“允许任意量通过”

#### 2 决定放什么新信息到“细胞状态”中

![image-20200229140537887](D:\Deep_Learning_PyTorch\docs\image\image-20200229140537887.png)

* $\sigma$ 为$sigmoid$函数
* Sigmoid层决定什么值需要更新，是对新信息的筛选
* Sigmoid 层输出0到1之间的概率值，描述每个部分有多少量可以通过。0代表“不许任何量通过”，1就指“允许任意量通过”
* $tanh$层创建一个新的候选值向量，激活函数。处于0、1之间
* 两式的参数不同，所以得到的信息是不一样的
* 上述2步是为状态更新做准备

#### 3 更新“细胞状态”

![image-20200229141107282](D:\Deep_Learning_PyTorch\docs\image\image-20200229141107282.png)

* 更新$C_{t-1}$为$C_t$
* 把旧状态与$f_t$相乘，丢弃掉需要丢弃的信息
* 加上$i_t$*$\hat{C}_t$ 。这就是新的候选值，根据我们决定更新每个状态的程度进行变化。
* $\hat{C}_t$为全部新信息

#### 4 基于“细胞状态”得到输出

![image-20200229141627633](D:\Deep_Learning_PyTorch\docs\image\image-20200229141627633.png)

* 首先运行一个$sigmoid$ 层得到$o_t$来确定细胞状态的哪个部分将输出

* 接着用$tanh$处理细胞状态(得到一个在-1到1之间的值)，再将它和sigmoid门的输出相乘，输出我们确定输出的那部分。

  

### LSTM的变体

#### 门层也会接受细胞状态的输入

![image-20200229142543142](D:\Deep_Learning_PyTorch\docs\image\image-20200229142543142.png)

#### 使用同一组权重

![image-20200229142635609](D:\Deep_Learning_PyTorch\docs\image\image-20200229142635609.png)

#### GRU



![image-20200229143315192](D:\Deep_Learning_PyTorch\docs\image\image-20200229143315192.png)

* 将忘记门和输入门合成了一个单一的更新门
* 同样还混合了细胞状态和隐藏状态，和其他一些改动。
* 比标准LSTM简单