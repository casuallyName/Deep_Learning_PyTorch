# 卷积神经网络与典型结构

## 卷积神经网络层级结构

![image-20200227160433760](D:\Deep_Learning_PyTorch\docs\image\image-20200227160433760.png)

一个卷积神经网络会包含：

* 数据输入层/ Input layer
* 卷积计算层/ CONV layer
* 激励层/ Activation layer
* 池化层/ Pooling layer
* 全连接层/ FC layer
* Batch Normalization层(可能有)

### 数据输入层/ Input layer

数据输入即对数据预处理，通常包括：

* 去均值，把输入数据各个维度都中心化到0

* 归一化，将幅度归一化到同样的范围

  ![image-20200227162301684](D:\Deep_Learning_PyTorch\docs\image\image-20200227162301684.png)

* PCA，降维

* 白化，对数据每个特征轴上的幅度归一化

![image-20200227162538337](D:\Deep_Learning_PyTorch\docs\image\image-20200227162538337.png)

### 卷积计算层/ CONV layer

![image-20200227163123839](D:\Deep_Learning_PyTorch\docs\image\image-20200227163123839.png)

每一个神经元可以看做一个filter，每个filter在计算时都对应一个滑动窗口，卷积核对划窗内的数据进行计算

滑窗每次滑动的距离成为步长（stride），当窗口滑到边缘是可能还需要填充数据（zero-padding）

![image-20200227163637846](D:\Deep_Learning_PyTorch\docs\image\image-20200227163637846.png)

kernel即卷积核，代表其所关注的内容。其输出结果称为feature map。

### 激励层/ Activation layer

激励层是把卷积层输出结果做非线性映射，常用的激活函数有：

![image-20200227164206491](D:\Deep_Learning_PyTorch\docs\image\image-20200227164206491.png)

其中，CNN应当慎用Sigmoid与tanh激活函数，其斜率只有在零左右很小一部分是存在的，剩下大部分都是0或近似为0，这会导致梯度消失，即神经元死掉。

### 池化层/ Pooling layer

池化层加载连续的卷积层之间，起作用是压缩数据量和参数量，减小过拟合

![image-20200227164627759](D:\Deep_Learning_PyTorch\docs\image\image-20200227164627759.png)

其中64为上层神经元个数（通道数），224X224为上层输出的feature map。

常见的两个池化层：

* Max pooling
* average pooling

<img src="D:\Deep_Learning_PyTorch\docs\image\image-20200227164708907.png" alt="image-20200227164708907" style="zoom:50%;" />

> 此图演示池化窗口为2X2，步长为2

### 全连接层/ FC layer

通常全连接层在卷积神经网络尾部，将输出结果做全连接，扁平化（二维）

## 卷积层可视化理解

<img src="D:\Deep_Learning_PyTorch\docs\image\image-20200227165641384.png" alt="image-20200227165641384" style="zoom: 80%;" />

随着卷积层输的加深，其提取到的特性人类越难理解。

## 训练算法

先定义Loss function，衡量和实际结果之间差距，找到最小化损失函数的W和b。

常用的算法：

* BP算法，利用链式求导法则，逐级相乘直到求解出dW和db。

* SGD算法，利用SGD/随机梯度下降，迭代和更新W和b

## 优缺点

优点：

* 共享卷积核，优化计算量

* 无需手动选取特征，训练好权重，即得特征

* 深层次的网络抽取图像信息丰富，表达效果好

缺点

* 需要调参，需要大样本量，GPU等硬件依赖
* 物理含义不明确，在高层卷积网络中其所抽取到的信息人类很难理解

## 过拟合

增加神经元可以提取到更多的特征，但当神经元过多时，就会有过拟合现象，即神经元关注的特征为无关特征。

Dropout(随机失活)正则化可以有效地防止过拟合，其原理为在卷积网络中随机关闭一些神经元，

![image-20200227170441833](D:\Deep_Learning_PyTorch\docs\image\image-20200227170441833.png)



