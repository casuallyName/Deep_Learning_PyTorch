<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

>注：文章知识点大多总结自《动手学习深度学习》，程序采用PyTorch模块演示。

# 线性回归
线性回归输出是一个连续值，因此适用于回归问题。回归问题在实际中很常见，如预测房屋价格、气温、销售额等连续值的问题。
与回归问题不同，分类问题中模型的最终输出是一个离散值。我们所说的图像分类、垃圾邮件识别、疾病检测等输出为离散值的问题都属于分类问题的范畴。逻辑回归（softmax）则适用于分类问题。
由于线性回归和逻辑回归都是单层神经网络，它们涉及的概念和技术同样适用于大多数的深度学习模型。这里先以线性回归为例，介绍大多数深度学习模型的基本要素和表示方法。

以一个简单的房屋价格预测为例来解释线性回归的基本要素。设$x$月的房屋的单位售出价格为$y$。现在需要建立基于输入$x$计算输出$y$的表达式，也就是模型（model）。线性回归假设输出与各个输入之间是线性关系：  
$$\hat{y} = xw   b$$
其中$w$是权重（weight），$b$是偏差（bias），且均为标量。它们是线性回归模型的参数（parameter）。模型输出$\hat{y}$是线性回归对真实价格$y$的预测或估计。
![](img/linear_0.png)

在模型训练中，通常允许它们之间有一定误差，但需要衡量价格预测值与真实值之间的误差。通常我们会选取一个非负数作为误差，且数值越小表示误差越小。一个常用的选择是平方函数。它在评估索引为$i$的样本误差的表达式为:
$$\ell^{(i)}(w, b) = \frac{1}{2} \left(\hat{y}^{(i)} - y^{(i)}\right)^2,$$
其中常数1⁄2使对平方项求导后的常数系数为1，这样在形式上稍微简单一些。显然，误差越小表示预测价格与真实价格越相近，且当二者相等时误差为0。

给定训练数据集，这个误差只与模型参数相关，因此我们将它记为以模型参数为参数的函数。

在机器学习里，将衡量误差的函数称为损失函数（loss function）。这里使用的平方误差函数也称为平方损失（square loss）。通常，用训练数据集中所有样本误差的平均来衡量模型预测的质量，定义为平均损失函数，即
$$\ell(w, b) =\frac{1}{n} \sum_{i=1}^n \ell^{(i)}(w, b) =\frac{1}{n} \sum_{i=1}^n \frac{1}{2}\left(x^{(i)} w   b - y^{(i)}\right)^2.$$
其中，$n$为样本总量。

在上述模型中，希望找到一组参数，记为$w^*, b^*$，使训练样本平均损失最小。

当模型和损失函数形式较为简单时，上面的误差最小化问题的解可以直接用公式表达出来。这类解叫作解析解(analytical solution)。使用的线性回归和平方误差刚好属于这个范畴。然而， 大多数深度学习模型并没有解析解，只能通过优化算法有限次迭代模型参数来尽可能降低损失函数的值。这类解叫作数值解(numerical solution)。 

在求数值解的优化算法中，小批量随机梯度下降(mini-batch stochastic gradient descent)在深度学习中被广泛使用。它的算法为:先选取一组模型参数的初始值，接下来对参数进行多次迭代，使每次迭代都可能降低损失函数的值。在每次迭代中，先随机均匀采样一个由固定数目训练数据样本所组成的小批量B(mini-batch)，然后求小批量中数据样本的平均损失有关模型参数的导数(梯度)，最后用此结果与预先设定的一个正数的乘积作为模型参数在本次迭代的减小量。 

在上面讨论的线性回归模型的训练过程中，模型的每个参数将作如下迭代:
$$
\begin{aligned}
w &\leftarrow w -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \frac{ \partial \ell^{(i)}(w, b)  }{\partial w} = w -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}x^{(i)} \left(x^{(i)} w   b - y^{(i)}\right),\\
b &\leftarrow b -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \frac{ \partial \ell^{(i)}(w, b)  }{\partial b} = b -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}\left(x^{(i)} w   b - y^{(i)}\right).
\end{aligned}
$$
在上式中，${|\mathcal{B}|}$代表每个小批量中的样本个数(批量大小，batch size)，η称作学习率(learning rate)并取正数。需要强调的是，这里的批量大小和学习率的值是人为设定的，并不是通过模型训练学习出来的，因此叫作超参数(hyperparameter)。超参数需要通过反复试错来找到超参数合适的值。在少数情况下，超参数也可以通过模型训练学出。 
模型训练完成后，将模型参数$w$，$b$在优化算法停止时的值分别记作$\hat{w}$，$\hat{b}$。注意，这里得到的并不一定是最小化损失函数的最优解$w^*, b^*$，而是对最优解的一个近似。然后就可以使用学习出的线性回归模型$x\hat{w}   \hat{b}$来预测训练数据集以外的房屋的价格走势。

# PyTorch实现简单的线性回归算法

## 1.准备数据

```python
# linspace可以生成0-100之间的均匀的100个数字
x = torch.linspace(0, 100).type(torch.FloatTensor)
# 随机生成100个满足标准正态分布的随机数，均值为0，方差为1.将这个数字乘以10，标准方差变为10
rand = torch.randn(100) * 10
# 将x和rand相加，得到伪造的标签数据y。所以(x,y)应能近似地落在y=x这条直线上
y = x   rand

# 划分数据集
x_train = x[: -10]
x_test = x[-10:]
y_train = y[: -10]
y_test = y[-10:]

# 将生成的训练数据点画在图上
plt.figure(figsize=(10, 8))  # 设定绘制窗口大小为10*8 inch
plt.plot(x_train.numpy(), y_train.numpy(), 'o')  # 绘制数据，考虑到x和y都是Variable，需要用data获取它们包裹的Tensor，并专成numpy
plt.xlabel('X')  # 添加X轴的标注
plt.ylabel('Y')  # 添加Y周的标注
plt.show()  # 将图形画在下面

```
![在这里插入图片描述](img/linear_1.tiff)



## 2.构造模型，计算损失函数
这里需要注意`expand_as`和`mul`方法的使用。
首先，`a`的维度为1，`x`是维度为100*1的Tensor，这两者因为维度不同不能直接相乘（矩阵知识）。所以要将`a`升维成1*1的Tensor。
expand_as(x)可以将张量升维成与x同维度的张量。所以如果a = 1, x为尺寸为100，那么，
 a.expand_as(x) $= (1, 1, \cdot\cdot\cdot, 1)^T$
 x * y为两个1维张量的乘积，计算结果：$(x * y)_i = x_i \cdot y_i$
 

```python

a = torch.rand(1, requires_grad=True)
b = torch.rand(1, requires_grad=True)
predictions = a.expand_as(x_train) * x_train   b.expand_as(x_train)
# print(predictions)
# tensor([ 0.4982,  0.8570,  1.2157,  1.5744,  1.9332,  2.2919,  2.6507,  3.0094,
#          3.3682,  3.7269,  4.0857,  4.4444,  4.8031,  5.1619,  5.5206,  5.8794,
#          6.2381,  6.5969,  6.9556,  7.3144,  7.6731,  8.0319,  8.3906,  8.7493,
#          9.1081,  9.4668,  9.8256, 10.1843, 10.5431, 10.9018, 11.2606, 11.6193,
#         11.9780, 12.3368, 12.6955, 13.0543, 13.4130, 13.7718, 14.1305, 14.4893,
#         14.8480, 15.2067, 15.5655, 15.9242, 16.2830, 16.6417, 17.0005, 17.3592,
#         17.7180, 18.0767, 18.4354, 18.7942, 19.1529, 19.5117, 19.8704, 20.2292,
#         20.5879, 20.9467, 21.3054, 21.6642, 22.0229, 22.3816, 22.7404, 23.0991,
#         23.4579, 23.8166, 24.1754, 24.5341, 24.8929, 25.2516, 25.6103, 25.9691,
#         26.3278, 26.6866, 27.0453, 27.4041, 27.7628, 28.1216, 28.4803, 28.8390,
#         29.1978, 29.5565, 29.9153, 30.2740, 30.6328, 30.9915, 31.3503, 31.7090,
#         32.0677, 32.4265], grad_fn=<AddBackward0>)

# 计算损失函数
loss = torch.mean((predictions - y_train) ** 2)
# 开始反向传播梯度
loss.backward()
#开始梯度下降，其中0.001为学习率
a.data.add_(- 0.001 * a.grad.data)
b.data.add_(- 0.001 * b.grad.data)

# 注
# 无法改变一个Variable，而只能对Variable的data属性做更改
# 所有函数加“_”意味着该方法需要更新调用者的数值。
```
## 3.训练模型训练模型

```python
a = torch.rand(1, requires_grad=True)  # 创建a变量，并随机赋值初始化
b = torch.rand(1, requires_grad=True)  # 创建b变量，并随机赋值初始化
print('Initial parameters:', [a, b])
learning_rate = 0.0001  # 设置学习率
# 开始迭代
for i in range(1000):
    # 计算在当前a、b条件下的模型预测数值
    predictions = a.expand_as(x_train) * x_train   b.expand_as(x_train)
    # 通过与标签数据y比较，计算误差
    loss = torch.mean((predictions - y_train) ** 2)
    # print('loss:', loss)
    # 对损失函数进行梯度反传
    loss.backward()
    # 利用上一步计算中得到的a的梯度信息更新a中的data数值
    a.data.add_(- learning_rate * a.grad.data)
    # 利用上一步计算中得到的b的梯度信息更新b中的data数值
    b.data.add_(- learning_rate * b.grad.data)
    # 清空存储在变量a，b中的梯度信息，以免在backward的过程中会反复不停地累加
    a.grad.data.zero_()  # 清空a的梯度数值
    b.grad.data.zero_()  # 清空b的梯度数值

x_data = x_train.data.numpy()  # 获得x包裹的数据
plt.figure(figsize=(10, 7))  # 设定绘图窗口大小
xplot, = plt.plot(x_data, y_train.numpy(), 'o')  # 绘制原始数据
yplot, = plt.plot(x_data, a.data.numpy() * x_data   b.data.numpy())  # 绘制拟合数据
plt.xlabel('X')  # 更改坐标轴标注
plt.ylabel('Y')  # 更改坐标轴标注
str1 = str(a.data.numpy()[0])   'x  '   str(b.data.numpy()[0])  # 图例信息
plt.legend([xplot, yplot], ['Data', str1])  # 绘制图例
plt.show()
```
![在这里插入图片描述](img/linear_2.tiff)

## 4.测试验证

```python
predictions = a.expand_as(x_test) * x_test   b.expand_as(x_test)  # 计算模型的预测结果
# print(predictions)  # 输出
# Initial parameters: [tensor([0.3117], requires_grad=True), tensor([0.8948], requires_grad=True)]
# tensor([88.3761, 89.3462, 90.3164, 91.2866, 92.2568, 93.2270, 94.1971, 95.1673,
#         96.1375, 97.1077], grad_fn=<AddBackward0>)
x_data = x_train.data.numpy()  # 获得x包裹的数据
x_pred = x_test.data.numpy()
plt.figure(figsize=(10, 7))  # 设定绘图窗口大小
plt.plot(x_data, y_train.data.numpy(), 'o')  # 绘制训练数据
plt.plot(x_pred, y_test.data.numpy(), 's')  # 绘制测试数据
x_data = np.r_[x_data, x_test.data.numpy()]
plt.plot(x_data, a.data.numpy() * x_data   b.data.numpy())  # 绘制拟合数据
plt.plot(x_pred, a.data.numpy() * x_pred   b.data.numpy(), 'o')  # 绘制预测数据
plt.xlabel('X')  # 更改坐标轴标注
plt.ylabel('Y')  # 更改坐标轴标注
str1 = str(a.data.numpy()[0])   'x  '   str(b.data.numpy()[0])  # 图例信息
plt.legend([xplot, yplot], ['Data', str1])  # 绘制图例
plt.show()
```
![在这里插入图片描述](img/linear_3.tiff)