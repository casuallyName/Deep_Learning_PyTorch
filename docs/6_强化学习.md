# 强化学习

## 马尔可夫决策过程(MDP)==？==

### 打折的未来奖励  

* 一个马尔科夫决策过程，它对应的奖励总和

$$
R = r_1+r_2+r_3+...+r_n
$$

* t时刻的未来奖励

  $$
  R_t = r_t+r_{t+1}+r_{t+2}+..+r_n​
  $$
  
* 无法确定在两次采取相同动作，agent能够获得相同的奖励，未来有不确定性，计算“打折的未来奖励”

  $$
  R_t=r_t+ \gamma r_{t+1}+ \gamma^2r_{t+2}+...+ \gamma^{n-t}r_n
  $$

  * gamma是一个0到1的值，使得我们更少地考虑哪些更长远未来的奖励

* $R_t* $可以用$R_{t+1}$来表示，写成递推式:

  $$
  R_t=r_t+\gamma(r_{t+1}+\gamma(r_{t+2}+...))=r_+t+\gamma R_{t+1}
  $$
  



## Q-learning

* $Q(s, a)$函数，用来表示智能体在$s$状态下采用$a$动作并在之后采取最优动作条件下的打折的未来奖励

  $$
  Q(s_t,a_t)=maxR_{t+1}
  $$
  

假设得到了Q-函数，你只要选取Q-函数值最大的动作(policy明确)
$$
\pi(s)=argmax_aQ(s,a)
$$


我们可以采用与打折的未来奖励相同的方式定义这一状态下的Q函数
$$
Q(s, a)=r+\gamma max_{a'}Q(s',a')
$$


贝尔曼公式实际非常合理。对于某个状态来讲，最大化未来奖励相当于最大化即刻奖励与下一状态最大未来奖励之和。

### Q-learning例子

![image-20200229202833717](image\image-20200229202833717.png)

==>

![image-20200229202858071](image\image-20200229202858071.png)

将agent处于任何一个位置，让其自己走，直到走到房间5，表示成功。为了能够走出去，将每一个节点设置一定的权重，能够到达5的边设置为100，其他不能的设置为0 ==>

![image-20200229203356615](image\image-20200229203356615.png)

Q-Learning中最重要的就是状态和动作，状态表示处于图中的那一节点，动作表示从一个节点到另一个节点

首先生成一个奖励矩阵，-1表示不可通过，0表示可通过，100表示到达终点： ==>

<img src="image\image-20200229203711903.png" alt="image-20200229203711903" style="zoom: 50%;" />

创建一个Q表，与R同阶，初始化为0矩阵，表示从state到另一个state能够获得的总的奖励的折现值

<img src="image\image-20200229203842586.png" alt="image-20200229203842586" style="zoom: 50%;" />

Q表中的值将根据$Q(s,a)=R(s,a)+\gamma·max_{\hat{a}}\{Q(\hat{s},\hat{a})\}$公式来进行更新

$s$ 表示当前状态

$a$ 表示当前动作

$\hat{s}$ 表示下一状态

$\hat{a}$ 表示下一动作

$Q$ 表示状态$s$下采取动作$a$能够获得的期望最大利益

$R$ 是立即获得的收益

$\gamma$ 为贪婪因子 $0<\gamma<1$,一般设置为0.8

![image-20200229204512632](image\image-20200229204512632.png)

<img src="image\image-20200229205745551.png" alt="image-20200229205745551" style="zoom:50%;" />

随机选择一个状态，比如1，查看状态所对应的R表，进行随机选择可到达的下一状态（3,5），例如选择5，则：
$$
Q(1,5)=R(1,5)+0.8*max{Q(5,1),Q(5,4),Q(5,5)}\\=100+0.8*max(0,0,0)\\=100
$$




<img src="image\image-20200229205306317.png" alt="image-20200229205306317" style="zoom:50%;" />

这样一次尝试结束，接下来再选择一个随机状态，比如（3，3）对应的下一个可到达状态有（1,2,4），随机选择，继续更新Q

<img src="image\image-20200229205323471.png" alt="image-20200229205323471" style="zoom:50%;" />

经过不停地迭代，最终得到完整的Q表，可画出如下示意图

![image-20200229205338661](image\image-20200229205338661.png)

## Deep Q-Network

![image-20200229211406025](image\image-20200229211406025.png)

* 采用连续两个游戏屏幕的像素，球速和方向等各种信息也可以表示出来

* 屏幕大小84*84，每个像素点有256个灰度值，总共256^(84*84*4)~10^67970种可能的状态
* Q-table有10^67970行，且非常稀疏（有很多状态遇不到！！）

#### 改造一下：

只接受一个状态作为输入，然后输出所有动作的分数（具体来讲是动作个数大小的向量），这样一次前向运算可以得到所有动作的得分

![image-20200229211755246](image\image-20200229211755246.png)

对网络做如上修改，一组state输入（四张图片），对应一组连续值

* 没有池化层
  
* 池化层带来位置不变性，会使我们的网络对于物体的位置不敏感，从而无法有效地识别游戏中球的位置
  
* 回归问题，损失函数

  <img src="image\image-20200229212724077.png" alt="image-20200229212724077" style="zoom:50%;" />



